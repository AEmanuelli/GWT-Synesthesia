{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eab07eee-59e5-48a6-bbeb-66500e875eaf",
   "metadata": {
    "id": "17deebf1-86e1-46db-bc91-820158d36bcc"
   },
   "source": [
    "# How to train your Globalâ€¯Workspace\n",
    "Benjamin Devillers\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruflab/shimmer-tutorials/blob/main/simple-shapes-dataset-training.ipynb)\n",
    "\n",
    "\n",
    "In this notebook, we will see how to use `shimmer` to build and train from scratch a Global Workspace on the Simple Shapes Dataset. We train a model than can translate visual images of shapes from the [simple-shapes-datset](https://github.com/ruflab/simple-shapes-dataset) to their proto-language (attributes).\n",
    "\n",
    "For this tutorial, we will need to install the [shimmer-ssd](https://github.com/ruflab/shimmer-ssd) package.\n",
    "\n",
    "# !pip install --force-reinstall \"git+https://github.com/AEmanuelli/shimmer-ssd.git\"\n",
    "\n",
    "# !pip install tensorboard\n",
    "\n",
    "This package depends on [simple-shapes-dataset](https://github.com/ruflab/simple-shapes-dataset) and provides all of its commands. You can then use all of its commands.\n",
    "\n",
    "For instance, we can download the dataset directly with:\n",
    "\n",
    "# !shapesd download\n",
    "\n",
    "Note that `shapesd download` automatically migrates the dataset so that it is correctly formatted. If you downloaded the dataset manually, use `shapesd migrate -p PATH_TO_DATASET` to migrate manually.\n",
    "\n",
    "from collections.abc import Mapping, Sequence\n",
    "from pathlib import Path\n",
    "from typing import Any, cast\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch import Callback, Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from shimmer import DomainModule, LossOutput\n",
    "from shimmer.modules.domain import DomainModule\n",
    "from shimmer.modules.global_workspace import GlobalWorkspace2Domains, SchedulerArgs\n",
    "from shimmer.modules.vae import (\n",
    "    VAE,\n",
    "    VAEDecoder,\n",
    "    VAEEncoder,\n",
    "    gaussian_nll,\n",
    "    kl_divergence_loss,\n",
    ")\n",
    "from shimmer_ssd import DEBUG_MODE, LOGGER, PROJECT_DIR\n",
    "from shimmer_ssd.config import DomainModuleVariant, LoadedDomainConfig, load_config\n",
    "from shimmer_ssd.dataset.pre_process import TokenizeCaptions\n",
    "from shimmer_ssd.logging import (\n",
    "    LogAttributesCallback,\n",
    "    LogGWImagesCallback,\n",
    "    LogVisualCallback,\n",
    "    batch_to_device,\n",
    ")\n",
    "from shimmer_ssd.modules.domains import load_pretrained_domains\n",
    "from shimmer_ssd.modules.domains.visual import VisualLatentDomainModule\n",
    "from shimmer_ssd.modules.vae import RAEDecoder, RAEEncoder\n",
    "from tokenizers.implementations.byte_level_bpe import ByteLevelBPETokenizer\n",
    "from torch import nn\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from simple_shapes_dataset import SimpleShapesDataModule, get_default_domains\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "## Config\n",
    "\n",
    "Let's first generate the config folder for the rest of the scripts.\n",
    "This will create a `config` folder with different yaml files used by the different scripts and in the notebook.\n",
    "\n",
    "# !ssd config create\n",
    "\n",
    "This will create a `config` folder. This contains many file, but in this tutorial, only `main.yaml` will interest us.\n",
    "\n",
    "You can start by taking a look at the default values which should be mostly set correctly for this tutorial. But you can try and make some changes to see the outcome.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Anytime you make a change to the config, don't forget to reload it with the following cell!\n",
    "</div>\n",
    "\n",
    "## Data format\n",
    "\n",
    "The dataloader provides the data in a specific format:\n",
    "\n",
    "```python\n",
    "domain_group = {\n",
    "    \"domain\": domain_data\n",
    "}\n",
    "batch = {\n",
    "    frozenset([\"domain\"]): domain_group\n",
    "}\n",
    "```\n",
    "* The **batch** is a dict that has frozensets of domains as keys, and a domain group as values.\n",
    "* The **domain group** is a dict that has domains (string) as keys, and the domain data as values. The data samples of every domain in a domain group is matched. This\n",
    "means that for a domain group that has 2 domains d1 and d2: `domain_group[\"d1\"][k]` is paired with `domain_group[\"d2\"][k]` for all `k`.\n",
    "\n",
    "This allows a batch to have several groups (of different domains) of paired data. For example, a batch with unpaired visual (domain \"v\"), unpaired attribute (domain \"attr\"), and paired visual and attribute will look like:\n",
    "```python\n",
    "batch = {\n",
    "    frozenset([\"v\"]): {\"v\": unpaired_visual_data},\n",
    "    frozenset([\"attr\"]): {\"attr\": unpaired_attribute_data},\n",
    "    frozenset([\"attr\", \"v\"]): {\"attr\": paired_attr_data, \"v\": paired_visual_data},\n",
    "}\n",
    "```\n",
    "\n",
    "This is useful to train the global workspace later. But this is also the format used to train the unimodal domains.\n",
    "\n",
    "Note that because all the data is paired in validation and test steps, the dataloader only returns one domain group with all paired domain:\n",
    "```python\n",
    "val_batch = {\"attr\": paired_attr_data, \"v\": paired_v_data}\n",
    "```\n",
    "\n",
    "## Train a Global Workspace\n",
    "\n",
    "Now that we trained our two unimodal modules, we will train the global workspace. For this training, we will use half of the paired 500,000 samples.\n",
    "To this extent, we need to create a split in the dataset. A dataset split depends on a seed and the proportion of each group of domain.\n",
    "We only need to generate this split once.\n",
    "\n",
    "This can be done with the `shapesd alignment add` command. It needs the following arguments:\n",
    "- `--dataset_path \"DATASET_PATH\"`: the location where the dataset is stored\n",
    "- `--seed SEED` the split seed\n",
    "- `--domain_alignment DOMAIN_1,DOMAIN_2,...DOMAIN_N PROP` the proportion for each domain group. This corresponds to what has been defined in `domain_proportion`\n",
    "\n",
    "When running this command, it will create a file containing the indices of the items available in the train set (update so that it matches what we set in the config file).\n",
    "\n",
    "# !shapesd alignment add --dataset_path \"simple_shapes_dataset\" --seed 0 --domain_alignment attr 1.0 --domain_alignment v 1.0 --domain_alignment attr,v 1.0\n",
    "\n",
    "This time, we will load the config from the extra file `train_gw.yaml`\n",
    "\n",
    "First, let's update `main.yaml` to use the same alignment split:\n",
    "```yaml\n",
    "domain_proportions:\n",
    "    -   domains: [\"v\"]  # unimodal visual passes use 100% of the available data\n",
    "        proportion: 1.0\n",
    "    -   domains: [\"attr\"]  # unimodal attr passes use 100% of the available data\n",
    "        proportion: 1.0\n",
    "    -   domains: [\"v\", \"attr\"]  # paired passes uses 50% of the available data\n",
    "        proportion: 0.5\n",
    "```\n",
    "\n",
    "let's change the selected domains:\n",
    "\n",
    "```yaml\n",
    "domains:\n",
    "    - checkpoint_path: \"./checkpoints/visual/version_0/last.ckpt\"  # update to the actual version\n",
    "      domain_type: v_latents\n",
    "    - checkpoint_path: \"./checkpoints/attr/version_0/last.ckpt\"  # update to the actual version\n",
    "      domain_type: attr\n",
    "```\n",
    "\n",
    "and let's define the global workspace dimenison to 12:\n",
    "```yaml\n",
    "global_workspace:\n",
    "    latent_dim: 12  \n",
    "    \n",
    "    loss_coefficients:\n",
    "        cycles: 1.0\n",
    "        contrastives: 0.1\n",
    "        demi_cycles: 1.0\n",
    "        translations: 1.0\n",
    "\n",
    "    encoders:\n",
    "        hidden_dim: 32\n",
    "        n_layers: 3\n",
    "\n",
    "    decoders:\n",
    "        hidden_dim: 32\n",
    "        n_layers: 3\n",
    "```\n",
    "\n",
    "Finally, let's load the config:\n",
    "\n",
    "config = load_config(\"./config\", use_cli=False, load_files=[\"train_gw.yaml\"])\n",
    "\n",
    "Skip the following cell if you have trained the unimodal module yourself. The next cell setups pretrained modules.\n",
    "\n",
    "### Run this if you did't train the modules\n",
    "\n",
    "# # Download checkpoints\n",
    "# !ssd download checkpoints\n",
    "# !mv checkpoints/checkpoints/* checkpoints/\n",
    "# !rm -rf checkpoints/checkpoints\n",
    "\n",
    "# # Extract visual latent from pretrained visual domain\n",
    "# !ssd extract v \"checkpoints/domain_v.ckpt\" -p \"simple_shapes_dataset\"\n",
    "\n",
    "my_hparams = {\"temperature\":0.5, \"alpha\": 2}\n",
    "\n",
    "\n",
    "# Update the config\n",
    "checkpoint_path = Path(\"./checkpoints\")\n",
    "config.domain_proportions = {\n",
    "    frozenset([\"v\"]): 1.0,\n",
    "    frozenset([\"attr\"]): 1.0,\n",
    "    frozenset([\"v\", \"attr\"]): 1.0,\n",
    "}\n",
    "\n",
    "config.domains = [\n",
    "    LoadedDomainConfig(\n",
    "        domain_type=DomainModuleVariant.v_latents,\n",
    "        checkpoint_path=checkpoint_path / \"domain_v.ckpt\",\n",
    "    ),\n",
    "    LoadedDomainConfig(\n",
    "        domain_type=DomainModuleVariant.attr_legacy,\n",
    "        checkpoint_path=checkpoint_path / \"domain_attr.ckpt\",\n",
    "        args=my_hparams,\n",
    "    ),\n",
    "]\n",
    "\n",
    "config.domain_data_args[\"v_latents\"][\"presaved_path\"] = \"domain_v.npy\"\n",
    "config.global_workspace.latent_dim = 12\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "def custom_collate(batch):\n",
    "    collated = default_collate(batch)\n",
    "    if isinstance(collated, dict) and \"attr\" in collated:\n",
    "        # Si \"attr\" est une liste et contient au moins deux tenseurs,\n",
    "        # on modifie uniquement le deuxiÃ¨me tenseur.\n",
    "        if isinstance(collated[\"attr\"], list) and len(collated[\"attr\"]) >= 2:\n",
    "            second_tensor = collated[\"attr\"][1]\n",
    "            if isinstance(second_tensor, torch.Tensor):\n",
    "                # On enlÃ¨ve les trois valeurs situÃ©es juste avant la derniÃ¨re.\n",
    "                if second_tensor.size(-1) >= 4:  # vÃ©rifie qu'il y a assez d'Ã©lÃ©ments\n",
    "                    collated[\"attr\"][1] = torch.cat(\n",
    "                        [second_tensor[..., : -4], second_tensor[..., -1:]], dim=-1\n",
    "                    )\n",
    "    return collated\n",
    "domain_classes = get_default_domains([\"v_latents\", \"attr\"])\n",
    "\n",
    "\n",
    "data_module = SimpleShapesDataModule(\n",
    "    config.dataset.path,\n",
    "    domain_classes,\n",
    "    config.domain_proportions,\n",
    "    batch_size=config.training.batch_size,\n",
    "    num_workers=config.training.num_workers,\n",
    "    seed=config.seed,\n",
    "    domain_args=config.domain_data_args,\n",
    "    collate_fn=custom_collate  # utilisation du collate personnalisÃ©\n",
    ")\n",
    "\n",
    "### Load the domains and train\n",
    "We can now load the pretrained unimodal modules\n",
    "\n",
    "# we load the pretrained domain modules and define the associated GW encoders and decoders\n",
    "domain_modules, gw_encoders, gw_decoders = load_pretrained_domains(\n",
    "    config.domains,\n",
    "    config.global_workspace.latent_dim,\n",
    "    config.global_workspace.encoders.hidden_dim,\n",
    "    config.global_workspace.encoders.n_layers,\n",
    "    config.global_workspace.decoders.hidden_dim,\n",
    "    config.global_workspace.decoders.n_layers,\n",
    ")\n",
    "\n",
    "Instanciate the global Workspace class\n",
    "\n",
    "def get_scheduler(optimizer: Optimizer) -> OneCycleLR:\n",
    "    return OneCycleLR(optimizer, config.training.optim.max_lr, config.training.max_steps)\n",
    "\n",
    "\n",
    "global_workspace = GlobalWorkspace2Domains(\n",
    "    domain_modules,\n",
    "    gw_encoders,\n",
    "    gw_decoders,\n",
    "    config.global_workspace.latent_dim,\n",
    "    config.global_workspace.loss_coefficients,\n",
    "    config.training.optim.lr,\n",
    "    config.training.optim.weight_decay,\n",
    "    scheduler=get_scheduler,\n",
    ")\n",
    "\n",
    "Add a Wandb logger to follow the training\n",
    "\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "\n",
    "# logger = TensorBoardLogger(\"logs\", name=\"gw\")\n",
    "logger_wandb = WandbLogger(name=\"gw_no_color\", project=\"shimmer-ssd\")\n",
    "logger = logger_wandb\n",
    "logger_wandb.log_hyperparams(my_hparams)\n",
    "\n",
    "\n",
    "# Get some image samples to log in tensorboard.\n",
    "train_samples = data_module.get_samples(\"train\", 32)\n",
    "val_samples = data_module.get_samples(\"val\", 32)\n",
    "\n",
    "# split the unique group in validation into individual groups for logging\n",
    "for domains in val_samples:\n",
    "    for domain in domains:\n",
    "        val_samples[frozenset([domain])] = {domain: val_samples[domains][domain]}\n",
    "    break\n",
    "# Create attr folder where we will save checkpoints\n",
    "(config.default_root_dir / \"gw\").mkdir(exist_ok=True)\n",
    "\n",
    "callbacks: list[Callback] = [\n",
    "    # Will log the validation ground-truth and reconstructions during training\n",
    "    LogGWImagesCallback(\n",
    "        val_samples,\n",
    "        log_key=\"images/val\",\n",
    "        mode=\"val\",\n",
    "        every_n_epochs=config.logging.log_val_medias_every_n_epochs,\n",
    "        filter=config.logging.filter_images,\n",
    "    ),\n",
    "    # Will log the training ground-truth and reconstructions during training\n",
    "    LogGWImagesCallback(\n",
    "        train_samples,\n",
    "        log_key=\"images/train\",\n",
    "        mode=\"train\",\n",
    "        every_n_epochs=config.logging.log_train_medias_every_n_epochs,\n",
    "        filter=config.logging.filter_images,\n",
    "    ),\n",
    "    # Save the checkpoints\n",
    "    ModelCheckpoint(\n",
    "        dirpath=config.default_root_dir / \"gw\" / f\"version_{logger.version}\",\n",
    "        filename=\"{epoch}\",\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=\"link\",\n",
    "        save_top_k=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "For the final model, let's save where the model is saved:\n",
    "\n",
    "gw_checkpoint = config.default_root_dir / \"gw\" / f\"version_{logger.version}\"\n",
    "print(gw_checkpoint)\n",
    "\n",
    "And train!\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    max_steps=config.training.max_steps,\n",
    "    default_root_dir=config.default_root_dir,\n",
    "    callbacks=callbacks,\n",
    "    precision=config.training.precision,\n",
    "    accelerator=config.training.accelerator,\n",
    "    devices=config.training.devices,\n",
    ")\n",
    "\n",
    "trainer.fit(global_workspace, data_module)\n",
    "trainer.validate(global_workspace, data_module, \"best\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
