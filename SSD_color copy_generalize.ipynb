{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "076316a4-8116-4476-9925-9326d71f556f",
   "metadata": {
    "id": "72c6c049-fa9d-4add-8156-7ec359734532"
   },
   "source": [
    "# How to train your Globalâ€¯Workspace\n",
    "Benjamin Devillers\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruflab/shimmer-tutorials/blob/main/simple-shapes-dataset-training.ipynb)\n",
    "\n",
    "\n",
    "In this notebook, we will see how to use `shimmer` to build and train from scratch a Global Workspace on the Simple Shapes Dataset. We train a model than can translate visual images of shapes from the [simple-shapes-datset](https://github.com/ruflab/simple-shapes-dataset) to their proto-language (attributes).\n",
    "\n",
    "For this tutorial, we will need to install the [shimmer-ssd](https://github.com/ruflab/shimmer-ssd) package.\n",
    "\n",
    "# !pip install --force-reinstall \"git+https://github.com/AEmanuelli/shimmer-ssd.git@SSD_color\"\n",
    "\n",
    "# !pip install tensorboard\n",
    "\n",
    "This package depends on [simple-shapes-dataset](https://github.com/ruflab/simple-shapes-dataset) and provides all of its commands. You can then use all of its commands.\n",
    "\n",
    "For instance, we can download the dataset directly with:\n",
    "\n",
    "# !shapesd download\n",
    "\n",
    "Note that `shapesd download` automatically migrates the dataset so that it is correctly formatted. If you downloaded the dataset manually, use `shapesd migrate -p PATH_TO_DATASET` to migrate manually.\n",
    "\n",
    "from collections.abc import Mapping, Sequence\n",
    "from pathlib import Path\n",
    "from typing import Any, cast\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch import Callback, Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from shimmer import DomainModule, LossOutput\n",
    "from shimmer.modules.domain import DomainModule\n",
    "from shimmer.modules.global_workspace import GlobalWorkspace2Domains, SchedulerArgs\n",
    "from shimmer.modules.vae import (\n",
    "    VAE,\n",
    "    VAEDecoder,\n",
    "    VAEEncoder,\n",
    "    gaussian_nll,\n",
    "    kl_divergence_loss,\n",
    ")\n",
    "from shimmer_ssd import DEBUG_MODE, LOGGER, PROJECT_DIR\n",
    "from shimmer_ssd.config import DomainModuleVariant, LoadedDomainConfig, load_config\n",
    "from shimmer_ssd.dataset.pre_process import TokenizeCaptions\n",
    "from shimmer_ssd.logging import (\n",
    "    LogAttributesCallback,\n",
    "    LogGWImagesCallback,\n",
    "    LogVisualCallback,\n",
    "    batch_to_device,\n",
    ")\n",
    "from shimmer_ssd.modules.domains import load_pretrained_domains\n",
    "from shimmer_ssd.modules.domains.visual import VisualLatentDomainModule\n",
    "from shimmer_ssd.modules.vae import RAEDecoder, RAEEncoder\n",
    "from tokenizers.implementations.byte_level_bpe import ByteLevelBPETokenizer\n",
    "from torch import nn\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from simple_shapes_dataset import SimpleShapesDataModule, get_default_domains\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "## Config\n",
    "\n",
    "Let's first generate the config folder for the rest of the scripts.\n",
    "This will create a `config` folder with different yaml files used by the different scripts and in the notebook.\n",
    "\n",
    "# !ssd config create\n",
    "\n",
    "This will create a `config` folder. This contains many file, but in this tutorial, only `main.yaml` will interest us.\n",
    "\n",
    "You can start by taking a look at the default values which should be mostly set correctly for this tutorial. But you can try and make some changes to see the outcome.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Anytime you make a change to the config, don't forget to reload it with the following cell!\n",
    "</div>\n",
    "\n",
    "# We don't use cli in the notebook, but consider using it in normal scripts.\n",
    "config = load_config(\"./config\", use_cli=False)\n",
    "\n",
    "## Data format\n",
    "\n",
    "The dataloader provides the data in a specific format:\n",
    "\n",
    "```python\n",
    "domain_group = {\n",
    "    \"domain\": domain_data\n",
    "}\n",
    "batch = {\n",
    "    frozenset([\"domain\"]): domain_group\n",
    "}\n",
    "```\n",
    "* The **batch** is a dict that has frozensets of domains as keys, and a domain group as values.\n",
    "* The **domain group** is a dict that has domains (string) as keys, and the domain data as values. The data samples of every domain in a domain group is matched. This\n",
    "means that for a domain group that has 2 domains d1 and d2: `domain_group[\"d1\"][k]` is paired with `domain_group[\"d2\"][k]` for all `k`.\n",
    "\n",
    "This allows a batch to have several groups (of different domains) of paired data. For example, a batch with unpaired visual (domain \"v\"), unpaired attribute (domain \"attr\"), and paired visual and attribute will look like:\n",
    "```python\n",
    "batch = {\n",
    "    frozenset([\"v\"]): {\"v\": unpaired_visual_data},\n",
    "    frozenset([\"attr\"]): {\"attr\": unpaired_attribute_data},\n",
    "    frozenset([\"attr\", \"v\"]): {\"attr\": paired_attr_data, \"v\": paired_visual_data},\n",
    "}\n",
    "```\n",
    "\n",
    "This is useful to train the global workspace later. But this is also the format used to train the unimodal domains.\n",
    "\n",
    "Note that because all the data is paired in validation and test steps, the dataloader only returns one domain group with all paired domain:\n",
    "```python\n",
    "val_batch = {\"attr\": paired_attr_data, \"v\": paired_v_data}\n",
    "```\n",
    "\n",
    "## Train a Global Workspace\n",
    "\n",
    "Now that we trained our two unimodal modules, we will train the global workspace. For this training, we will use half of the paired 500,000 samples.\n",
    "To this extent, we need to create a split in the dataset. A dataset split depends on a seed and the proportion of each group of domain.\n",
    "We only need to generate this split once.\n",
    "\n",
    "This can be done with the `shapesd alignment add` command. It needs the following arguments:\n",
    "- `--dataset_path \"DATASET_PATH\"`: the location where the dataset is stored\n",
    "- `--seed SEED` the split seed\n",
    "- `--domain_alignment DOMAIN_1,DOMAIN_2,...DOMAIN_N PROP` the proportion for each domain group. This corresponds to what has been defined in `domain_proportion`\n",
    "\n",
    "When running this command, it will create a file containing the indices of the items available in the train set (update so that it matches what we set in the config file).\n",
    "\n",
    "# !shapesd alignment add --dataset_path \"simple_shapes_dataset\" --seed 0 --domain_alignment attr 1.0 --domain_alignment v 1.0 --domain_alignment attr,v 1.0\n",
    "\n",
    "This time, we will load the config from the extra file `train_gw.yaml`\n",
    "\n",
    "First, let's update `main.yaml` to use the same alignment split:\n",
    "```yaml\n",
    "domain_proportions:\n",
    "    -   domains: [\"v\"]  # unimodal visual passes use 100% of the available data\n",
    "        proportion: 1.0\n",
    "    -   domains: [\"attr\"]  # unimodal attr passes use 100% of the available data\n",
    "        proportion: 1.0\n",
    "    -   domains: [\"v\", \"attr\"]  # paired passes uses 50% of the available data\n",
    "        proportion: 0.5\n",
    "```\n",
    "\n",
    "let's change the selected domains:\n",
    "\n",
    "```yaml\n",
    "domains:\n",
    "    - checkpoint_path: \"./checkpoints/visual/version_0/last.ckpt\"  # update to the actual version\n",
    "      domain_type: v_latents\n",
    "    - checkpoint_path: \"./checkpoints/attr/version_0/last.ckpt\"  # update to the actual version\n",
    "      domain_type: attr\n",
    "```\n",
    "\n",
    "and let's define the global workspace dimenison to 12:\n",
    "```yaml\n",
    "global_workspace:\n",
    "    latent_dim: 12  \n",
    "    \n",
    "    loss_coefficients:\n",
    "        cycles: 1.0\n",
    "        contrastives: 0.1\n",
    "        demi_cycles: 1.0\n",
    "        translations: 1.0\n",
    "\n",
    "    encoders:\n",
    "        hidden_dim: 32\n",
    "        n_layers: 3\n",
    "\n",
    "    decoders:\n",
    "        hidden_dim: 32\n",
    "        n_layers: 3\n",
    "```\n",
    "\n",
    "Finally, let's load the config:\n",
    "\n",
    "config = load_config(\"./config\", use_cli=False, load_files=[\"train_gw.yaml\"])\n",
    "\n",
    "Skip the following cell if you have trained the unimodal module yourself. The next cell setups pretrained modules.\n",
    "\n",
    "### Run this if you did't train the modules\n",
    "\n",
    "# # Download checkpoints\n",
    "# !ssd download checkpoints\n",
    "# !mv checkpoints/checkpoints/* checkpoints/\n",
    "# !rm -rf checkpoints/checkpoints\n",
    "\n",
    "# # Extract visual latent from pretrained visual domain\n",
    "# !ssd extract v \"checkpoints/domain_v.ckpt\" -p \"simple_shapes_dataset\"\n",
    "\n",
    "my_hparams = {\"temperature\": 2.0, \"alpha\": 0.5}\n",
    "\n",
    "# Update the config\n",
    "checkpoint_path = Path(\"./checkpoints\")\n",
    "config.domain_proportions = {\n",
    "    frozenset([\"v\"]): 1.0,\n",
    "    frozenset([\"attr\"]): 1.0,\n",
    "    frozenset([\"v\", \"attr\"]): 1.0,\n",
    "}\n",
    "\n",
    "config.domains = [\n",
    "    LoadedDomainConfig(\n",
    "        domain_type=DomainModuleVariant.v_latents,\n",
    "        checkpoint_path=checkpoint_path / \"domain_v.ckpt\",\n",
    "    ),\n",
    "    LoadedDomainConfig(\n",
    "        domain_type=DomainModuleVariant.attr_legacy,\n",
    "        checkpoint_path=checkpoint_path / \"domain_attr.ckpt\",\n",
    "        args=my_hparams,\n",
    "    ),\n",
    "]\n",
    "\n",
    "config.domain_data_args[\"v_latents\"][\"presaved_path\"] = \"domain_v.npy\"\n",
    "config.global_workspace.latent_dim = 12\n",
    "\n",
    "### Load the domains and train\n",
    "We can now load the pretrained unimodal modules\n",
    "\n",
    "# we load the pretrained domain modules and define the associated GW encoders and decoders\n",
    "domain_modules, gw_encoders, gw_decoders = load_pretrained_domains(\n",
    "    config.domains,\n",
    "    config.global_workspace.latent_dim,\n",
    "    config.global_workspace.encoders.hidden_dim,\n",
    "    config.global_workspace.encoders.n_layers,\n",
    "    config.global_workspace.decoders.hidden_dim,\n",
    "    config.global_workspace.decoders.n_layers,\n",
    ")\n",
    "\n",
    "Instanciate the global Workspace class\n",
    "\n",
    "def get_scheduler(optimizer: Optimizer) -> OneCycleLR:\n",
    "    return OneCycleLR(optimizer, config.training.optim.max_lr, config.training.max_steps)\n",
    "\n",
    "\n",
    "global_workspace = GlobalWorkspace2Domains(\n",
    "    domain_modules,\n",
    "    gw_encoders,\n",
    "    gw_decoders,\n",
    "    config.global_workspace.latent_dim,\n",
    "    config.global_workspace.loss_coefficients,\n",
    "    config.training.optim.lr,\n",
    "    config.training.optim.weight_decay,\n",
    "    scheduler=get_scheduler,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "domain_classes = get_default_domains([\"v_latents\", \"attr\"])\n",
    "\n",
    "data_module = SimpleShapesDataModule(\n",
    "    config.dataset.path,\n",
    "    domain_classes,\n",
    "    config.domain_proportions,\n",
    "    batch_size=config.training.batch_size,\n",
    "    num_workers=config.training.num_workers,\n",
    "    seed=config.seed,\n",
    "    domain_args=config.domain_data_args,\n",
    ")\n",
    "\n",
    "Add a Wandb logger to follow the training\n",
    "\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "# from tensorboard import TensorBoardLogger\n",
    "# logger = TensorBoardLogger(\"logs\", name=\"gw\")\n",
    "\n",
    "\n",
    "logger_wandb = WandbLogger(name=f\"gw_with_color_alpha[]\", project=\"shimmer-ssd\")\n",
    "# tensorboard_logger = TensorBoardLogger(\"logs\", name=\"gw\")\n",
    "\n",
    "logger = logger_wandb\n",
    "\n",
    "logger_wandb.log_hyperparams(my_hparams)\n",
    "\n",
    "# Get some image samples to log in tensorboard.\n",
    "train_samples = data_module.get_samples(\"train\", 32)\n",
    "val_samples = data_module.get_samples(\"val\", 32)\n",
    "\n",
    "# split the unique group in validation into individual groups for logging\n",
    "for domains in val_samples:\n",
    "    for domain in domains:\n",
    "        val_samples[frozenset([domain])] = {domain: val_samples[domains][domain]}\n",
    "    break\n",
    "# Create attr folder where we will save checkpoints\n",
    "(config.default_root_dir / \"gw\").mkdir(exist_ok=True)\n",
    "\n",
    "callbacks: list[Callback] = [\n",
    "    # Will log the validation ground-truth and reconstructions during training\n",
    "    LogGWImagesCallback(\n",
    "        val_samples,\n",
    "        log_key=\"images/val\",\n",
    "        mode=\"val\",\n",
    "        every_n_epochs=config.logging.log_val_medias_every_n_epochs,\n",
    "        filter=config.logging.filter_images,\n",
    "    ),\n",
    "    # Will log the training ground-truth and reconstructions during training\n",
    "    LogGWImagesCallback(\n",
    "        train_samples,\n",
    "        log_key=\"images/train\",\n",
    "        mode=\"train\",\n",
    "        every_n_epochs=config.logging.log_train_medias_every_n_epochs,\n",
    "        filter=config.logging.filter_images,\n",
    "    ),\n",
    "    # Save the checkpoints\n",
    "    ModelCheckpoint(\n",
    "        dirpath=config.default_root_dir / \"gw\" / f\"version_{logger.version}\",\n",
    "        filename=\"{epoch}\",\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=\"link\",\n",
    "        save_top_k=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "For the final model, let's save where the model is saved:\n",
    "\n",
    "gw_checkpoint = config.default_root_dir / \"gw\" / f\"version_{logger.version}\"\n",
    "print(gw_checkpoint)\n",
    "\n",
    "And train!\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    max_steps=config.training.max_steps,\n",
    "    default_root_dir=config.default_root_dir,\n",
    "    callbacks=callbacks,\n",
    "    precision=config.training.precision,\n",
    "    accelerator=config.training.accelerator,\n",
    "    devices=config.training.devices,\n",
    ")\n",
    "\n",
    "trainer.fit(global_workspace, data_module)\n",
    "trainer.validate(global_workspace, data_module, \"best\")\n",
    "\n",
    "#Quel type de modÃ¨le ?\n",
    "MODEL_TYPE = \"sans_couleur\"\n",
    "\n",
    "# And now we load the GW checkpoint\n",
    "checkpoint_path = Path(\"./checkpoints\")\n",
    "# We don't use cli in the notebook, but consider using it in normal scripts.\n",
    "config = load_config(\"./config\", use_cli=False)\n",
    "\n",
    "\n",
    "if MODEL_TYPE == \"full_attr\":\n",
    "    domain_type = DomainModuleVariant.attr\n",
    "    checkpoint = checkpoint_path / \"gw-attr-v-all-paired-data.ckpt\"\n",
    "    config.global_workspace.encoders.n_layers = 3\n",
    "    config.global_workspace.decoders.n_layers = 3\n",
    "    attributes = torch.tensor(\n",
    "        [[x * 2 - 1, y * 2 - 1, size * 2 - 1, rotx, roty, color_r * 2 - 1, color_g * 2 - 1, color_b * 2 - 1]]\n",
    "    )\n",
    "else : \n",
    "    domain_type = DomainModuleVariant.attr_legacy\n",
    "    checkpoint = checkpoint_path / \"gw/version_None/epoch=660.ckpt\"\n",
    "    attributes = torch.tensor(\n",
    "        [[x * 2 - 1, y * 2 - 1, size * 2 - 1, rotx, roty ]]#, color_r * 2 - 1, color_g * 2 - 1, color_b * 2 - 1]]\n",
    "    )\n",
    "    config.global_workspace.encoders.n_layers = 3\n",
    "    config.global_workspace.decoders.n_layers = 3\n",
    "\n",
    "\n",
    "#CEST UN TEST\n",
    "\n",
    "# Update the config\n",
    "checkpoint_path = Path(\"./checkpoints\")\n",
    "\n",
    "config.domain_proportions = {\n",
    "    frozenset([\"v\"]): 1.0,\n",
    "    frozenset([\"attr\"]): 1.0,\n",
    "    frozenset([\"v\", \"attr\"]): 1.0,\n",
    "}\n",
    "\n",
    "config.domains = [\n",
    "    LoadedDomainConfig(\n",
    "        domain_type=DomainModuleVariant.v_latents,\n",
    "        checkpoint_path=checkpoint_path / \"domain_v.ckpt\",\n",
    "    ),\n",
    "    LoadedDomainConfig(\n",
    "        domain_type=domain_type,\n",
    "        checkpoint_path=checkpoint_path / \"domain_attr.ckpt\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "config.domain_data_args[\"v_latents\"][\"presaved_path\"] = \"domain_v.npy\"\n",
    "config.global_workspace.latent_dim = 12\n",
    "\n",
    "domain_modules, gw_encoders, gw_decoders = load_pretrained_domains(\n",
    "    config.domains,\n",
    "    config.global_workspace.latent_dim,\n",
    "    config.global_workspace.encoders.hidden_dim,\n",
    "    config.global_workspace.encoders.n_layers,\n",
    "    config.global_workspace.decoders.hidden_dim,\n",
    "    config.global_workspace.decoders.n_layers,\n",
    ")\n",
    "\n",
    "global_workspace = GlobalWorkspace2Domains.load_from_checkpoint(\n",
    "    checkpoint,\n",
    "    domain_mods=domain_modules,\n",
    "    gw_encoders=gw_encoders,\n",
    "    gw_decoders=gw_decoders,\n",
    ")\n",
    "\n",
    "### Run this if you did't train the model\n",
    "\n",
    "# # Update the config\n",
    "# checkpoint_path = Path(\"./checkpoints\")\n",
    "\n",
    "# config.domain_proportions = {\n",
    "#     frozenset([\"v\"]): 1.0,\n",
    "#     frozenset([\"attr\"]): 1.0,\n",
    "#     frozenset([\"v\", \"attr\"]): 1.0,\n",
    "# }\n",
    "\n",
    "# config.domains = [\n",
    "#     LoadedDomainConfig(\n",
    "#         domain_type=DomainModuleVariant.v_latents,\n",
    "#         checkpoint_path=checkpoint_path / \"domain_v.ckpt\",\n",
    "#     ),\n",
    "#     LoadedDomainConfig(\n",
    "#         domain_type=DomainModuleVariant.attr_legacy,\n",
    "#         checkpoint_path=checkpoint_path / \"domain_attr.ckpt\",\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# config.domain_data_args[\"v_latents\"][\"presaved_path\"] = \"domain_v.npy\"\n",
    "# config.global_workspace.latent_dim = 12\n",
    "# # And now we load the GW checkpoint\n",
    "# checkpoint_path = Path(\"./checkpoints\")\n",
    "# # checkpoint = checkpoint_path / \"gw-attr-v-half-paired-data.ckpt\"\n",
    "# checkpoint  = \"/home/alexis/Desktop/checkpoints/gw/version_None/epoch=120-v1.ckpt\"\n",
    "# # we load the pretrained domain modules and define the associated GW encoders and decoders\n",
    "# domain_modules, gw_encoders, gw_decoders = load_pretrained_domains(\n",
    "#     config.domains,\n",
    "#     config.global_workspace.latent_dim,\n",
    "#     config.global_workspace.encoders.hidden_dim,\n",
    "#     config.global_workspace.encoders.n_layers,\n",
    "#     config.global_workspace.decoders.hidden_dim,\n",
    "#     config.global_workspace.decoders.n_layers,\n",
    "# )\n",
    "\n",
    "# global_workspace = GlobalWorkspace2Domains.load_from_checkpoint(\n",
    "#     checkpoint,\n",
    "#     domain_mods=domain_modules,\n",
    "#     gw_encoders=gw_encoders,\n",
    "#     gw_decoders=gw_decoders,\n",
    "# )\n",
    "\n",
    "## Play with the global workspace\n",
    "\n",
    "import io\n",
    "import math\n",
    "%pip install ipywidgets ipympl\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact, interact_manual\n",
    "from PIL import Image\n",
    "from shimmer_ssd.logging import attribute_image_grid\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "from simple_shapes_dataset.cli import generate_image\n",
    "%matplotlib widget\n",
    "\n",
    "# !conda install -c conda-forge ipympl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "global_workspace.to(device)\n",
    "\n",
    "cat2idx = {\"Diamond\": 0, \"Egg\": 1, \"Triangle\": 2}\n",
    "\n",
    "\n",
    "def get_image(cat, x, y, size, rot, color_r, color_g, color_b):\n",
    "    fig, ax = plt.subplots(figsize=(32, 32), dpi=1)\n",
    "    # The dataset generatoion tool has function to generate a matplotlib shape\n",
    "    # from the attributes.\n",
    "    generate_image(\n",
    "        ax,\n",
    "        cat2idx[cat],\n",
    "        [int(x * 18 + 7), int(y * 18 + 7)],\n",
    "        size * 7 + 7,\n",
    "        rot * 2 * math.pi,\n",
    "        np.array([color_r * 255, color_g * 255, color_b * 255]),\n",
    "        imsize=32,\n",
    "    )\n",
    "    ax.set_facecolor(\"black\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    # Return this as a PIL Image.\n",
    "    # This is to have the same dpi as saved images\n",
    "    # otherwise matplotlib will render this in very high quality\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    image = Image.open(buf)\n",
    "    plt.close(fig)\n",
    "    return image\n",
    "\n",
    "\n",
    "@interact(\n",
    "    cat=[\"Triangle\", \"Egg\", \"Diamond\"],\n",
    "    x=(0, 1, 0.1),\n",
    "    y=(0, 1, 0.1),\n",
    "    rot=(0, 1, 0.1),\n",
    "    size=(0, 1, 0.1),\n",
    "    color_r=(0, 1, 0.1),\n",
    "    color_g=(0, 1, 0.1),\n",
    "    color_b=(0, 1, 0.1),\n",
    ")\n",
    "def play_with_gw(\n",
    "    cat: str = \"Triangle\",\n",
    "    x: float = 0.5,\n",
    "    y: float = 0.5,\n",
    "    rot: float = 0.5,\n",
    "    size: float = 0.5,\n",
    "    color_r: float = 1,\n",
    "    color_g: float = 0,\n",
    "    color_b: float = 0,\n",
    "):\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    image = get_image(cat, x, y, size, rot, color_r, color_g, color_b)\n",
    "    axes[0].set_facecolor(\"black\")\n",
    "    axes[0].set_title(\"Original image from attributes\")\n",
    "    axes[0].set_xticks([])\n",
    "    axes[0].set_yticks([])\n",
    "    axes[0].imshow(image)\n",
    "\n",
    "    # normalize the attribute for the global workspace.\n",
    "    category = one_hot(torch.tensor([cat2idx[cat]]), 3)\n",
    "    rotx = math.cos(rot * 2 * math.pi)\n",
    "    roty = math.sin(rot * 2 * math.pi)\n",
    "    if MODEL_TYPE == \"full_attr\":\n",
    "        attributes = torch.tensor(\n",
    "            [[x * 2 - 1, y * 2 - 1, size * 2 - 1, rotx, roty, color_r * 2 - 1, color_g * 2 - 1, color_b * 2 - 1]]\n",
    "        )\n",
    "    else:\n",
    "        attributes = torch.tensor(\n",
    "            [[x * 2 - 1, y * 2 - 1, size * 2 - 1, rotx, roty]]  # , color_r * 2 - 1, color_g * 2 - 1, color_b * 2 - 1]]\n",
    "        )\n",
    "    samples = [category.to(device), attributes.to(device)]\n",
    "    attr_gw_latent = global_workspace.gw_mod.encode({\"attr\": global_workspace.encode_domain(samples, \"attr\")})\n",
    "    gw_latent = global_workspace.gw_mod.fuse(\n",
    "        attr_gw_latent, {\"attr\": torch.ones(attr_gw_latent[\"attr\"].size(0)).to(device)}\n",
    "    )\n",
    "    decoded_latents = global_workspace.gw_mod.decode(gw_latent)[\"v_latents\"]\n",
    "    decoded_images = (\n",
    "        global_workspace.domain_mods[\"v_latents\"]\n",
    "        .decode_images(decoded_latents)[0]\n",
    "        .permute(1, 2, 0)\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    axes[1].imshow(decoded_images)\n",
    "    axes[1].set_xticks([])\n",
    "    axes[1].set_yticks([])\n",
    "    axes[1].set_title(\"Translated image through GW\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[18], line 11\n",
    "      1 trainer = Trainer(\n",
    "      2     logger=logger,\n",
    "      3     max_steps=config.training.max_steps,\n",
    "   (...)\n",
    "      8     devices=config.training.devices,\n",
    "      9 )\n",
    "---> 11 trainer.fit(global_workspace, data_module)\n",
    "     12 trainer.validate(global_workspace, data_module, \"best\")\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:539, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n",
    "    537 self.state.status = TrainerStatus.RUNNING\n",
    "    538 self.training = True\n",
    "--> 539 call._call_and_handle_interrupt(\n",
    "    540     self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
    "    541 )\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\n",
    "     45     if trainer.strategy.launcher is not None:\n",
    "     46         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
    "---> 47     return trainer_fn(*args, **kwargs)\n",
    "     49 except _TunerExitException:\n",
    "     50     _call_teardown_hook(trainer)\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:575, in Trainer._fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n",
    "    568 assert self.state.fn is not None\n",
    "    569 ckpt_path = self._checkpoint_connector._select_ckpt_path(\n",
    "    570     self.state.fn,\n",
    "    571     ckpt_path,\n",
    "    572     model_provided=True,\n",
    "    573     model_connected=self.lightning_module is not None,\n",
    "    574 )\n",
    "--> 575 self._run(model, ckpt_path=ckpt_path)\n",
    "    577 assert self.state.stopped\n",
    "    578 self.training = False\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:982, in Trainer._run(self, model, ckpt_path)\n",
    "    977 self._signal_connector.register_signal_handlers()\n",
    "    979 # ----------------------------\n",
    "    980 # RUN THE TRAINER\n",
    "    981 # ----------------------------\n",
    "--> 982 results = self._run_stage()\n",
    "    984 # ----------------------------\n",
    "    985 # POST-Training CLEAN UP\n",
    "    986 # ----------------------------\n",
    "    987 log.debug(f\"{self.__class__.__name__}: trainer tearing down\")\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1026, in Trainer._run_stage(self)\n",
    "   1024         self._run_sanity_check()\n",
    "   1025     with torch.autograd.set_detect_anomaly(self._detect_anomaly):\n",
    "-> 1026         self.fit_loop.run()\n",
    "   1027     return None\n",
    "   1028 raise RuntimeError(f\"Unexpected state {self.state}\")\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216, in _FitLoop.run(self)\n",
    "    214 try:\n",
    "    215     self.on_advance_start()\n",
    "--> 216     self.advance()\n",
    "    217     self.on_advance_end()\n",
    "    218 except StopIteration:\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:455, in _FitLoop.advance(self)\n",
    "    453 with self.trainer.profiler.profile(\"run_training_epoch\"):\n",
    "    454     assert self._data_fetcher is not None\n",
    "--> 455     self.epoch_loop.run(self._data_fetcher)\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:151, in _TrainingEpochLoop.run(self, data_fetcher)\n",
    "    149 try:\n",
    "    150     self.advance(data_fetcher)\n",
    "--> 151     self.on_advance_end(data_fetcher)\n",
    "    152 except StopIteration:\n",
    "    153     break\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:370, in _TrainingEpochLoop.on_advance_end(self, data_fetcher)\n",
    "    366 if not self._should_accumulate():\n",
    "    367     # clear gradients to not leave any unused memory during validation\n",
    "    368     call._call_lightning_module_hook(self.trainer, \"on_validation_model_zero_grad\")\n",
    "--> 370 self.val_loop.run()\n",
    "    371 self.trainer.training = True\n",
    "    372 self.trainer._logger_connector._first_loop_iter = first_loop_iter\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:179, in _no_grad_context.<locals>._decorator(self, *args, **kwargs)\n",
    "    177     context_manager = torch.no_grad\n",
    "    178 with context_manager():\n",
    "--> 179     return loop_run(self, *args, **kwargs)\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:151, in _EvaluationLoop.run(self)\n",
    "    149         self.on_iteration_done()\n",
    "    150 self._store_dataloader_outputs()\n",
    "--> 151 return self.on_run_end()\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:291, in _EvaluationLoop.on_run_end(self)\n",
    "    288 self.trainer._logger_connector._evaluation_epoch_end()\n",
    "    290 # hook\n",
    "--> 291 self._on_evaluation_epoch_end()\n",
    "    293 logged_outputs, self._logged_outputs = self._logged_outputs, []  # free memory\n",
    "    294 # include any logged outputs on epoch_end\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:370, in _EvaluationLoop._on_evaluation_epoch_end(self)\n",
    "    367 trainer = self.trainer\n",
    "    369 hook_name = \"on_test_epoch_end\" if trainer.testing else \"on_validation_epoch_end\"\n",
    "--> 370 call._call_callback_hooks(trainer, hook_name)\n",
    "    371 call._call_lightning_module_hook(trainer, hook_name)\n",
    "    373 trainer._logger_connector.on_epoch_end()\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:222, in _call_callback_hooks(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\n",
    "    220     if callable(fn):\n",
    "    221         with trainer.profiler.profile(f\"[Callback]{callback.state_key}.{hook_name}\"):\n",
    "--> 222             fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
    "    224 if pl_module:\n",
    "    225     # restore current_fx when nested context\n",
    "    226     pl_module._current_fx_name = prev_fx_name\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/shimmer_ssd/logging.py:706, in LogGWImagesCallback.on_validation_epoch_end(self, trainer, pl_module)\n",
    "    700 if (\n",
    "    701     self.every_n_epochs is None\n",
    "    702     or trainer.current_epoch % self.every_n_epochs != 0\n",
    "    703 ):\n",
    "    704     return\n",
    "--> 706 return self.on_callback(trainer.loggers, pl_module)\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/shimmer_ssd/logging.py:640, in LogGWImagesCallback.on_callback(self, loggers, pl_module)\n",
    "    634             samples[1] = torch.cat(\n",
    "    635                 [samples[1][..., :-1], \n",
    "    636                  torch.tensor([1, 0, 0], device=device).expand(32, 3), \n",
    "    637                  samples[1][..., -1:]], dim=-1\n",
    "    638             )\n",
    "    639             print(f\"Shape of samples[1]: {samples[1].shape}\")\n",
    "--> 640         self.log_samples(\n",
    "    641             logger,\n",
    "    642             pl_module,\n",
    "    643             samples,\n",
    "    644             domain,\n",
    "    645             log_name,\n",
    "    646         )\n",
    "    647 for domains, preds in predictions[\"cycles\"].items():\n",
    "    648     domain_from = \",\".join(domains)\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/shimmer_ssd/logging.py:754, in LogGWImagesCallback.log_samples(self, logger, pl_module, samples, domain, mode)\n",
    "    752     self.log_visual_samples(logger, module.decode_images(samples), mode)\n",
    "    753 case \"attr\":\n",
    "--> 754     self.log_attribute_samples(logger, samples, mode)\n",
    "    755 case \"t\":\n",
    "    756     self.log_text_samples(logger, samples, mode)\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/shimmer_ssd/logging.py:775, in LogGWImagesCallback.log_attribute_samples(self, logger, samples, mode)\n",
    "    769 def log_attribute_samples(\n",
    "    770     self,\n",
    "    771     logger: Logger,\n",
    "    772     samples: Any,\n",
    "    773     mode: str,\n",
    "    774 ) -> None:\n",
    "--> 775     image = attribute_image_grid(\n",
    "    776         samples,\n",
    "    777         image_size=self.image_size,\n",
    "    778         ncols=self.ncols,\n",
    "    779     )\n",
    "    780     log_image(logger, f\"{self.log_key}/{mode}\", image, self.get_step())\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/shimmer_ssd/logging.py:258, in attribute_image_grid(samples, image_size, ncols)\n",
    "    255 sizes = attributes.size.detach().cpu().numpy()\n",
    "    256 rotations = attributes.rotation.detach().cpu().numpy()\n",
    "--> 258 return get_attribute_figure_grid(\n",
    "    259     categories,\n",
    "    260     locations,\n",
    "    261     sizes,\n",
    "    262     rotations,\n",
    "    263     colors,\n",
    "    264     image_size,\n",
    "    265     ncols,\n",
    "    266     padding=2,\n",
    "    267 )\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/shimmer_ssd/logging.py:216, in get_attribute_figure_grid(categories, locations, sizes, rotations, colors, image_size, ncols, padding)\n",
    "    214             break\n",
    "    215         ax = plt.subplot(gs[i, j])\n",
    "--> 216         generate_image(\n",
    "    217             ax,\n",
    "    218             categories[k],\n",
    "    219             locations[k],\n",
    "    220             sizes[k],\n",
    "    221             rotations[k],\n",
    "    222             colors[k],\n",
    "    223             image_size,\n",
    "    224         )\n",
    "    225         ax.set_facecolor(\"black\")\n",
    "    226 image = get_pil_image(figure)\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/simple_shapes_dataset/cli/utils.py:134, in generate_image(ax, cls, location, scale, rotation, color, imsize)\n",
    "    132     patch = get_diamond_patch(location, scale, rotation, color)\n",
    "    133 elif cls == 1:\n",
    "--> 134     patch = get_egg_patch(location, scale, rotation, color)\n",
    "    135 elif cls == 2:\n",
    "    136     patch = get_triangle_patch(location, scale, rotation, color)\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/simple_shapes_dataset/cli/utils.py:116, in get_egg_patch(location, scale, rotation, color)\n",
    "     97 codes = [\n",
    "     98     mpath.Path.MOVETO,\n",
    "     99     mpath.Path.CURVE4,\n",
    "   (...)\n",
    "    110     mpath.Path.CURVE4,\n",
    "    111 ]\n",
    "    112 path = mpath.Path(\n",
    "    113     get_transformed_coordinates(coordinates, origin, scale, rotation),\n",
    "    114     codes,\n",
    "    115 )\n",
    "--> 116 patch = patches.PathPatch(path, facecolor=color)\n",
    "    117 return patch\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/matplotlib/patches.py:1008, in PathPatch.__init__(self, path, **kwargs)\n",
    "    999 @_docstring.interpd\n",
    "   1000 def __init__(self, path, **kwargs):\n",
    "   1001     \"\"\"\n",
    "   1002     *path* is a `.Path` object.\n",
    "   1003 \n",
    "   (...)\n",
    "   1006     %(Patch:kwdoc)s\n",
    "   1007     \"\"\"\n",
    "-> 1008     super().__init__(**kwargs)\n",
    "   1009     self._path = path\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/matplotlib/patches.py:85, in Patch.__init__(self, edgecolor, facecolor, color, linewidth, linestyle, antialiased, hatch, fill, capstyle, joinstyle, **kwargs)\n",
    "     83 else:\n",
    "     84     self.set_edgecolor(edgecolor)\n",
    "---> 85     self.set_facecolor(facecolor)\n",
    "     87 self._linewidth = 0\n",
    "     88 self._unscaled_dash_pattern = (0, None)  # offset, dash\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/matplotlib/patches.py:404, in Patch.set_facecolor(self, color)\n",
    "    396 \"\"\"\n",
    "    397 Set the patch face color.\n",
    "    398 \n",
    "   (...)\n",
    "    401 color : :mpltype:`color` or None\n",
    "    402 \"\"\"\n",
    "    403 self._original_facecolor = color\n",
    "--> 404 self._set_facecolor(color)\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/matplotlib/patches.py:392, in Patch._set_facecolor(self, color)\n",
    "    390     color = mpl.rcParams['patch.facecolor']\n",
    "    391 alpha = self._alpha if self._fill else 0\n",
    "--> 392 self._facecolor = colors.to_rgba(color, alpha)\n",
    "    393 self.stale = True\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/matplotlib/colors.py:316, in to_rgba(c, alpha)\n",
    "    314     rgba = None\n",
    "    315 if rgba is None:  # Suppress exception chaining of cache lookup failure.\n",
    "--> 316     rgba = _to_rgba_no_colorcycle(c, alpha)\n",
    "    317     try:\n",
    "    318         _colors_full_map.cache[c, alpha] = rgba\n",
    "\n",
    "File ~/Desktop/.conda/lib/python3.11/site-packages/matplotlib/colors.py:414, in _to_rgba_no_colorcycle(c, alpha)\n",
    "    412     c = c[:3] + (alpha,)\n",
    "    413 if any(elem < 0 or elem > 1 for elem in c):\n",
    "--> 414     raise ValueError(\"RGBA values should be within 0-1 range\")\n",
    "    415 return c\n",
    "\n",
    "ValueError: RGBA values should be within 0-1 range\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
